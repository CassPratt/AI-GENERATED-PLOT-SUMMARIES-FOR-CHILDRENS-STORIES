{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8614691b",
   "metadata": {},
   "source": [
    "# Image captioning model - Tune\n",
    "Following guide from [HuggingFace: Image captioning](https://huggingface.co/docs/transformers/main/en/tasks/image_captioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f43458",
   "metadata": {},
   "source": [
    "## Libraries & packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839da9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate -q\n",
    "#!pip install jiwer -q\n",
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fea200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b62dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d619d81b9f7477cbc132e7fdcd70480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bcdc6",
   "metadata": {},
   "source": [
    "##  Import dataset\n",
    "Generate a {image-caption} pairs metadata file in JSON format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f50a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder with images\n",
    "img_path = '../Datasets/childrens-books'\n",
    "\n",
    "# Path to file with images' description\n",
    "# Original is CSV format\n",
    "img_path_csv = img_path + '/childrens-books-captions.csv'\n",
    "# Generated JSON file path\n",
    "img_path_json = img_path + '/metadata.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a629853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a CSV to JSON\n",
    "# Takes the file paths as arguments\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "     \n",
    "    # create a list\n",
    "    data = []\n",
    "     \n",
    "    # Open a csv reader called DictReader\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "        csvReader = csv.DictReader(csvf)\n",
    "         \n",
    "        # Add each row to data list as dictionary\n",
    "        for row in csvReader:\n",
    "            data.append(row)\n",
    " \n",
    "    # Open a json writer, and use the json.dumps()\n",
    "    # function to dump data\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf:\n",
    "        for item in data:\n",
    "            jsonf.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfa9ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the make_json function\n",
    "# Saves the json file from csv original file\n",
    "make_json(img_path_csv, img_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a8a74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bdca390d854c9e8195cc5b28d67fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9caf1f851144f1904b156495d1173d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41907b9e756b4370a9c378a2be4e5652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091380dca1354ad2b87ae00738b1ef35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110982e1c4524d0bb91fbcd50f4df05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "img_ds = load_dataset('imagefolder', data_dir = img_path)\n",
    "#img_ds = Dataset.from_dict(img_path_json)\n",
    "# Print image dataset info to confirm it was correctly created\n",
    "img_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92dbda",
   "metadata": {},
   "source": [
    "## Split dataset into train & test sets\n",
    "Split dataset into train and test, test size of 25% of total images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test, test size of 25% of total images\n",
    "img_ds_split = img_ds['train'].train_test_split(test_size=0.25)\n",
    "train_ds = img_ds_split['train']\n",
    "test_ds = img_ds_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb62f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images\n",
    "def plot_images(images, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb696361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some images from train dataset\n",
    "sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "plot_images(sample_images_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b67f6",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "Since the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor class associated with the model we are about to fine-tune\n",
    "checkpoint = 'microsoft/git-large-r-coco'\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51989f8",
   "metadata": {},
   "source": [
    "The processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbaccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process train and test datasets\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c0c3a",
   "metadata": {},
   "source": [
    "## Load a base model\n",
    "Load the “microsoft/git-large-r-coco” into a AutoModelForCausalLM object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f97bb",
   "metadata": {},
   "source": [
    "## Define evaluation type\n",
    "Image captioning models are typically evaluated with the Rouge Score or Word Error Rate. We will use the **Word Error Rate (WER)**.\n",
    "\n",
    "**Limitation of WER:** provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5baa7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WER \n",
    "wer = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate with WER\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db485b8a",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57096087",
   "metadata": {},
   "source": [
    "Define the training arguments using TrainingArguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base model name\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-childrensbooks\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,    # For GPU only\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c409378",
   "metadata": {},
   "source": [
    "Pass the arguments along with the datasets and the model to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca095cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebca583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376151c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f5a2b-ec15-42fa-bb26-eb739953b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare image for the model and caption it\n",
    "def caption_image(img_path):\n",
    "    # Prepare image for the model\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    # Call generate and decode the predictions\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    pixel_values = inputs.pixel_values\n",
    "    \n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Return image and caption\n",
    "    return image, generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d07c9f-c1c7-4e8e-a0f5-d57544ca2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of images\n",
    "imgs_validation = ['0545077974.jpg','0545533643.jpg','0545834910.jpg','0553508504.jpg','0670887102.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923a087-da33-449f-880b-95921f642f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption test images\n",
    "for img in imgs_validation:\n",
    "    image, generated_caption = caption_image(img)\n",
    "    image.show()\n",
    "    print('\\033[1mGenerated caption: \\033[0m',generated_caption)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5ec61-714a-4ce3-9308-aa87816320a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
